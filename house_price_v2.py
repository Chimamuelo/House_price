# -*- coding: utf-8 -*-
"""House_Price_v2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H-KNyzfNjWF1WL9iISpignRjlOSsOPUY
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

train_set=pd.read_csv("/content/train.csv")
test_set=pd.read_csv("/content/test.csv")



train_set.head()

train_set.drop('Id',axis=1,inplace=True)
test_id=test_set['Id'].astype(int)
test_set.drop('Id',axis=True,inplace=True)

nan_features=np.array(np.where(train_set.isna().sum()>1000))
nan_features=list(nan_features)
type(nan_features)

for i in nan_features:
   train_set.drop(train_set.columns[i],axis=1,inplace=True)
   test_set.drop(test_set.columns[i],axis=1,inplace=True)

train_set.head()

obj_df=train_set.select_dtypes(['object'])
for column in obj_df.columns:
    ax=obj_df[column].value_counts().sort_values().plot(kind='bar')
    plt.title(column)
    plt.show()



train_set.shape

"""Drop highly imbalanced columns"""

percentage_list=[]
for column in train_set.columns:
    perc=train_set[column].value_counts(normalize=True)
    perc=perc.max()
    percentage_list.append([column,perc])

#percentage_list
#train_set.drop('Street',axis=1,inplace=True)

Ok=[]
for column,value in percentage_list:
   print(column,value)
    
   if float(value)>.90:
       try:
           train_set.drop(column,axis=1,inplace=True)
           test_set.drop(column,axis=1,inplace=True)
       except:
           Ok.append(column)

print(train_set.shape,test_set.shape)

test_set.isna().sum()

for column in test_set.columns:

    test_set[column].fillna(test_set[column].mode()[0],inplace=True)
    train_set[column].fillna(train_set[column].mode()[0],inplace=True)

train_set.head()

test_set.shape

train_set['MSSubClass']=train_set['MSSubClass'].astype('object')
test_set['MSSubClass']=test_set['MSSubClass'].astype('object')

df=train_set.select_dtypes('object')
df.shape

to_drop=[]
for column in df.columns:
  if len(train_set[column].unique())!=len(test_set[column].unique()):
    plt.figure(1)
    
    plt.subplot(2,2,1)
    plt.title(column)
    to_drop.append(column)
    
    train_set[column].value_counts().sort_values().plot(kind='bar')
    plt.subplot(2,2,2)
    test_set[column].value_counts().sort_values().plot(kind='bar')
    plt.title(column)
    plt.show()
  else:
    
    print('ok')

train_set=train_set[train_set['HouseStyle']!='2.5Fin']
train_set=train_set[train_set['Exterior1st']!='Stone']
train_set=train_set[train_set['Exterior1st']!='ImStucc']
train_set=train_set[train_set['Exterior2nd']!='Other']



for column in to_drop:
  train_set.drop(column,axis=1,inplace=True)
  test_set.drop(column,axis=1,inplace=True)

df=train_set.select_dtypes('object')
df.shape

for column in df.columns:
  train_set=pd.concat([train_set,pd.get_dummies(train_set[column],prefix=column)],axis=1)
  train_set.drop(column,axis=1,inplace=True)
  test_set=pd.concat([test_set,pd.get_dummies(test_set[column],prefix=column)],axis=1)
  test_set.drop(column,axis=1,inplace=True)

train_set.head()

test_set.head()

test_set.isna().sum()

for column in train_set.columns:
  print(column)

label=train_set['SalePrice']
train_set.drop('SalePrice',axis=1,inplace=True)

"""#Make Pipeline"""

from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler
from sklearn.pipeline import Pipeline,make_pipeline
from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score, ShuffleSplit
from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold, KFold
from sklearn.impute import SimpleImputer
import xgboost
# evaluation
from sklearn.metrics import f1_score, accuracy_score
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeRegressor

imp=SimpleImputer(missing_values=np.nan,strategy='mean')
imp.fit(train_set)
X=imp.transform(train_set)
y=imp.transform(test_set)

X_df=pd.DataFrame(X,columns=train_set.columns)
X_df.head()

y_df=pd.DataFrame(y,columns=test_set.columns)
y_df.head()



#imp=Normalizer()
#imp.fit(X_df)
#X=imp.transform(X_df)
#y=imp.transform(y_df)

X_df=pd.DataFrame(X,columns=train_set.columns)
X_df.head()

y_df=pd.DataFrame(y,columns=test_set.columns)
y_df.head()
for column in y_df.columns:
    X_df[column].astype(int)
    y_df[column].astype(int)



X=round(.8*len(X_df))
x_train=X_df[:X]
x_test=X_df[X:]
y_train=label[:X]
y_test=label[X:]

y_train.head()

x_train.dtypes

y_test.shape

x_test.shape

params={'max_depth':[2,10,20],
        'learning_rate':[0.1,.01,.05], 
        'n_estimators':[50,100,200,300,400]
        
}
#         
#}
#params={'svc__gamma':['auto','scale']}

duplicate_columns = x_train.columns[x_train.columns.duplicated()]

duplicate_columns

grid=xgboost.XGBRegressor(n_estimators=200)
#grid=GridSearchCV(model,param_grid=params,cv=5)



grid.fit(x_train,y_train)

from sklearn.metrics import mean_squared_error

score=cross_val_score(grid,x_test,y_test,cv=5,scoring='max_error')
score

pred=grid.predict(x_test)

pred

score=mean_squared_error(y_test,pred)
score

result=grid.predict(y_df)
result=result.round().astype(int)

len(test_id)

len(result)



test_id=pd.concat([test_id,pd.DataFrame(data=result,columns=['SalePrice'])],axis=1)

test_id

test_id.dropna()

test_id.Id.astype(int)

test_id.to_csv('Coding_nomads.csv',index=False)